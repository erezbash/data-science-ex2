# Final Report Titanic: Machine Learning from Disaster
## Erez Bashari 
## Yakir Zana

# First Try: NaiveBayes

## [link NB.RMD](NB.Rmd)

## Pre-processing:
* read train.csv file
* Check the datatypes
* convert Survived column and Pclass column to factors
* remove from features:PassengerId, Name, Ticket, Cabin
* we decided to remove Cabin bacuse of the many N/A values and see that it really help to get better accuracy

## Naive Bayes:
It is a classification technique based on Bayesâ€™ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.

## [link NB.csv](NB.csv)

## upload result to the kaggle Competition
![pic](/picture/NB_Score.PNG)


# Second Try: C5.0

## [link C5.RMD](C5.Rmd)

## Pre-processing:
* read train.csv file
* Check the datatypes
* convert Survived column and Pclass column to factors
* remove from features:PassengerId, Name, Ticket, Cabin
* we decided to remove Cabin bacuse of the many N/A values and see that it really help to get better accuracy

## C5:
is an algorithm used to generate a decision tree that can be used for classification.
builds decision trees from a set of training data using the concept of information entropy( the average amount of information produced by a stochastic source of data).

## tide the data two times: the first is for categorial data and the second for numeric data:

![pic](/picture/C5_plot_1.PNG)

![pic](/picture/C5_plot_2.PNG)

## [link C5.csv](C5.csv)

## upload result to the kaggle Competition
![pic](/picture/C50.PNG)


# Third Try: Random Forest

```{r}
dfTrain <- read.csv("Titanic/train.csv",na.strings = "")
dfTest <- read.csv("Titanic/test.csv",na.strings = "")
```


# Load required packages
```{r}
library(rpart)
library(randomForest)
library(party)
```


# Join together the test and train sets for make pred for NAs fields
```{r}
dfTest$Survived <- NA
combind <- rbind(dfTrain, dfTest)
```


# Convert Name to a string
```{r}
combind$Name <- as.character(combind$Name)
```


# Create new variable: Title
```{r}
combind$Title <- sapply(combind$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})
combind$Title <- sub(' ', '', combind$Title)
```


# Combind small grupe of titles to one gruop
```{r}
combind$Title[combind$Title %in% c('Mme', 'Mlle')] <- 'Mlle'
combind$Title[combind$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] <- 'Lady'
combind$Title[combind$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir'
```


# Convert to a factor
```{r}
combind$Title <- factor(combind$Title)
```


# Fill in Age NAs with rpart for better predict
```{r}
Agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title,
                data=combind[!is.na(combind$Age),], method="anova")
combind$Age[is.na(combind$Age)] <- predict(Agefit, combind[is.na(combind$Age),])
```


# Fill in Fare NAs with rpart for better predict
```{r}
Farefit <- rpart(Fare ~ Pclass + Sex + SibSp + Parch + Age + Embarked + Title,
                data=combind[!is.na(combind$Fare),], method="anova")
combind$Fare[is.na(combind$Fare)] <- predict(Farefit, combind[is.na(combind$Fare),])
```


# Split Again to test and train 
```{r}
train <- combind[1:891,]
test <- combind[892:1309,]
```


# Build Random Forest
```{r}
fit <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title,
               data = train, controls=cforest_unbiased(ntree=2000, mtry=5)) 
```


# Bind PassengerId to the servived prediction and make csv
```{r}
Prediction <- predict(fit, test, OOB=TRUE, type = "response")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "forest.csv", row.names = FALSE)
```




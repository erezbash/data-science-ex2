# Final Report Titanic: Machine Learning from Disaster
## Erez Bashari 
## Yakir Zana

# First Try: NaiveBayes

## [link NB.RMD](NB.Rmd)

## Pre-processing:
* read train.csv file
* Check the datatypes
* convert Survived column and Pclass column to factors
* remove from features:PassengerId, Name, Ticket, Cabin
* we decided to remove Cabin bacuse of the many N/A values and see that it really help to get better accuracy

## Naive Bayes:
It is a classification technique based on Bayesâ€™ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.

## [link NB.csv](NB.csv)

## upload result to the kaggle Competition
![pic](/picture/NB_Score.PNG)


# Second Try: C5.0

## [link C5.RMD](C5.Rmd)

## Pre-processing:
* read train.csv file
* Check the datatypes
* convert Survived column and Pclass column to factors
* remove from features:PassengerId, Name, Ticket, Cabin
* we decided to remove Cabin bacuse of the many N/A values and see that it really help to get better accuracy

## C5:
is an algorithm used to generate a decision tree that can be used for classification.
builds decision trees from a set of training data using the concept of information entropy( the average amount of information produced by a stochastic source of data).

## tide the data two times: the first is for categorial data and the second for numeric data:

![pic](/picture/C5_plot_1.PNG)

![pic](/picture/C5_plot_2.PNG)

## [link C5.csv](C5.csv)

## upload result to the kaggle Competition
![pic](/picture/C50.PNG)


# Third Try: Random Forest

# Final Report Titanic: Machine Learning from Disaster
## Erez Bashari 
## Yakir Zana

# First Try: NaiveBayes

## [link NB.RMD](NB.Rmd)

## Pre-processing:
* read train.csv file
* Check the datatypes
* convert Survived column and Pclass column to factors
* remove from features:PassengerId, Name, Ticket, Cabin
* we decided to remove Cabin bacuse of the many N/A values and see that it really help to get better accuracy

## Naive Bayes:
It is a classification technique based on Bayesâ€™ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.

## [link NB.csv](NB.csv)

## upload result to the kaggle Competition
![pic](/picture/NB_Score.PNG)


# Second Try: C5.0

## [link C5.RMD](C5.Rmd)

## Pre-processing:
* read train.csv file
* Check the datatypes
* convert Survived column and Pclass column to factors
* remove from features:PassengerId, Name, Ticket, Cabin
* we decided to remove Cabin bacuse of the many N/A values and see that it really help to get better accuracy

## C5:
is an algorithm used to generate a decision tree that can be used for classification.
builds decision trees from a set of training data using the concept of information entropy( the average amount of information produced by a stochastic source of data).

## tide the data two times: the first is for categorial data and the second for numeric data:

![pic](/picture/C5_plot_1.PNG)

![pic](/picture/C5_plot_2.PNG)

## [link C5.csv](C5.csv)

## upload result to the kaggle Competition
![pic](/picture/C50.PNG)


# Third Try: Random Forest

## [link C5.RMD](C5.Rmd)

## Pre-processing:
* read train.csv file
* read test.csv file
* join together the test and train sets for make pred for NAs fields
* convert Name to a string for spliting
* create new variable: Title, that come from the name and make him a factor.
* fill Age and Fare NAs with prdeditction in rpart at the combind data.

## Random Forest:
Random Forests grows many classification trees. To classify a new object from an input vector, put the input vector down each of the trees in the forest. Each tree gives a classification, and we say the tree "votes" for that class. The forest chooses the classification having the most votes (over all the trees in the forest).


## [link forest.csv](forest.csv)

## upload result to the kaggle Competition

